{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300fb101",
   "metadata": {
    "id": "300fb101"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5dff1f",
   "metadata": {
    "id": "4e5dff1f"
   },
   "source": [
    "Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "idZbYSJuNPx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idZbYSJuNPx-",
    "outputId": "0f653bd8-fb67-4aba-e96c-d75738eefa78",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-2.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\lesly\\anaconda3\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\lesly\\anaconda3\\lib\\site-packages (2.0.43)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\lesly\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lesly\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (6.3.0)\n",
      "Collecting kagglesdk<1.0,>=0.1.15 (from kaggle)\n",
      "  Downloading kagglesdk-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (25.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (5.29.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from python-dateutil->kaggle) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from requests->kaggle) (2026.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lesly\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading kaggle-2.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading kagglesdk-0.1.16-py3-none-any.whl (160 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.8/2.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 8.3 MB/s  0:00:00\n",
      "Installing collected packages: psycopg2-binary, kagglesdk, kaggle\n",
      "\n",
      "   ------------- -------------------------- 1/3 [kagglesdk]\n",
      "   ------------- -------------------------- 1/3 [kagglesdk]\n",
      "   ---------------------------------------- 3/3 [kaggle]\n",
      "\n",
      "Successfully installed kaggle-2.0.0 kagglesdk-0.1.16 psycopg2-binary-2.9.11\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías que nos permitirá conectarse a PostgreSQL.\n",
    "!pip install kaggle pandas sqlalchemy psycopg2-binary openpyxl python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vi9ZZY84QUfG",
   "metadata": {
    "id": "vi9ZZY84QUfG"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport psycopg2\nimport zipfile\nimport os\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nfrom sqlalchemy import create_engine\nfrom dotenv import load_dotenv\n\n# Cargamos las variables de entorno del archivo .env\n# (Kaggle y PostgreSQL — nunca hardcodeadas en el código)\nload_dotenv()"
  },
  {
   "cell_type": "markdown",
   "id": "L5YvWPaUS4EU",
   "metadata": {
    "id": "L5YvWPaUS4EU"
   },
   "source": [
    "### Parte 1: Descarga mediante API de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27102c9f-6199-4d59-bcf0-3616db2c6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el dataset y la carpeta de destino\n",
    "dataset_name = \"jenifergrategarro/dataset-public-investments-in-peru\"\n",
    "ruta_data = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "j2ZsubLWRzsz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "j2ZsubLWRzsz",
    "outputId": "e6b0ff7f-a489-4fd3-83e2-bb2c3dbd15d8"
   },
   "outputs": [],
   "source": [
    "#Autenticamos la API\n",
    "api=KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b22203b-e247-40c3-af6f-8cb0cebf069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la carpeta DATA donde se alojará la información de Kaggle\n",
    "if not os.path.exists(ruta_data):\n",
    "    os.makedirs(ruta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ff4ed2-430c-4ab5-ab46-a320f02f0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando el dataset 'jenifergrategarro/dataset-public-investments-in-peru' de Kaggle...\n",
      "Dataset URL: https://www.kaggle.com/datasets/jenifergrategarro/dataset-public-investments-in-peru\n",
      "Completado!\n"
     ]
    }
   ],
   "source": [
    "# Descargamos y descomprimimos directamente\n",
    "print(f\"Descargando el dataset '{dataset_name}' de Kaggle...\")\n",
    "api.dataset_download_files(dataset_name, path=ruta_data, unzip=True)\n",
    "print(\"Completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f267f-77ba-46bd-b59f-eda520092ce0",
   "metadata": {},
   "source": [
    "### Parte 2: Unificar datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937b2c78",
   "metadata": {
    "id": "937b2c78",
    "outputId": "3aa11a44-6df8-4e55-9c84-cc1be748dd6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 26 archivos. Unificando...\n"
     ]
    }
   ],
   "source": [
    "ruta_data = \"data\"\n",
    "\n",
    "# Leemos la descarga de Kaggle\n",
    "archivos = [f for f in os.listdir(ruta_data) if f.endswith('.xlsx')]\n",
    "print(f\"Se encontraron {len(archivos)} archivos. Unificando...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865af56-0dbc-47f5-ae93-33ff414c5467",
   "metadata": {},
   "outputs": [],
   "source": "lista_dataframes = []\n\nfor archivo in archivos:\n    ruta_completa = os.path.join(ruta_data, archivo)\n    print(f\"Procesando: {archivo}\")\n    df_temp = pd.read_excel(ruta_completa)\n    lista_dataframes.append(df_temp)"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09f3bcb5-cff3-49e6-b175-998fd5bc5aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hecho. Unificación completada\n"
     ]
    }
   ],
   "source": [
    "# Concatenamos todos los dataframes\n",
    "df_maestro = pd.concat(lista_dataframes, ignore_index=True)\n",
    "print(\"Hecho. Unificación completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5629cea-a48c-4676-bb4e-60e08316010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡El archivo df_maestro tiene 381323 filas y 16 columnas.\n"
     ]
    }
   ],
   "source": [
    "# Limpieza\n",
    "df_maestro.columns = df_maestro.columns.str.lower().str.replace(' ', '_')\n",
    "df_maestro = df_maestro.drop_duplicates()\n",
    "\n",
    "print(f\"\\n El archivo df_maestro tiene {df_maestro.shape[0]} filas y {df_maestro.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da002487-d258-4b6d-9888-163e5407a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo maestro se ha guardado como: 'maestro_inversiones_peru.csv'\n"
     ]
    }
   ],
   "source": [
    "# Exportamos el archivo resultante\n",
    "nombre_salida = \"maestro_inversiones_peru.csv\"\n",
    "df_maestro.to_csv(nombre_salida, index=False)\n",
    "print(f\"El archivo maestro se ha guardado como: '{nombre_salida}'\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "4misyojm3hl",
   "source": "# Visualizando el archivo maestro\ndf_maestro.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c1949414",
   "metadata": {
    "id": "c1949414"
   },
   "source": "### Parte 3: Normalización y modelado de dimensiones"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63579ac2",
   "metadata": {
    "id": "63579ac2"
   },
   "outputs": [],
   "source": [
    "# Cargamos la base maestria con el nombre df\n",
    "df = pd.read_csv(\"maestro_inversiones_peru.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c29eee",
   "metadata": {
    "id": "76c29eee"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TABLA 1: DIMENSIÓN GEOGRAFÍA\n",
    "# ==========================================\n",
    "# 1. Filtramos las columnas y quitamos vacíos\n",
    "df_geografia = df[['ubigeo', 'departamento', 'provincia', 'distrito']].dropna(subset=['ubigeo'])\n",
    "\n",
    "# 2. Eliminamos duplicados basados ÚNICAMENTE en la columna 'ubigeo'\n",
    "df_geografia = df_geografia.drop_duplicates(subset=['ubigeo'], keep='first')\n",
    "\n",
    "# 3. Renombramos y convertimos a número entero\n",
    "df_geografia = df_geografia.rename(columns={'ubigeo': 'ubigeo_id', 'departamento': 'region'})\n",
    "df_geografia['ubigeo_id'] = df_geografia['ubigeo_id'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2a806ba",
   "metadata": {
    "id": "d2a806ba"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TABLA 2: DIMENSIÓN SECTORES\n",
    "# ==========================================\n",
    "df_sectores = df[['sector']].drop_duplicates().dropna().reset_index(drop=True)\n",
    "\n",
    "# Crear un ID numérico para cada sector (1, 2, 3...)\n",
    "df_sectores['sector_id'] = df_sectores.index + 1\n",
    "df_sectores = df_sectores.rename(columns={'sector': 'nombre_sector'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea09e32",
   "metadata": {
    "id": "9ea09e32"
   },
   "outputs": [],
   "source": "# ==========================================\n# TABLA 3: TABLA CENTRAL PROYECTOS\n# ==========================================\n# Primero cruzamos el df original con nuestra nueva tabla de sectores para traernos el \"sector_id\"\ndf_cruce = df.merge(df_sectores, left_on='sector', right_on='nombre_sector', how='left')\n\n# Seleccionamos solo las columnas necesarias (deben coincidir con las del SQL)\ncolumnas_proyectos = [\n    'código_único_de_inversión', 'nombre_de_la_inversión', 'fecha_de_registro',\n    'sector_id', 'ubigeo', 'monto_viable', 'costo_actualizado', 'beneficiarios', 'estado_de_la_inversión'\n]\ndf_proyectos = df_cruce[columnas_proyectos].drop_duplicates(subset=['código_único_de_inversión']).dropna(subset=['código_único_de_inversión'])\n\n# Renombramos según nuestro diseño SQL\ndf_proyectos = df_proyectos.rename(columns={\n    'código_único_de_inversión': 'proyecto_id',\n    'nombre_de_la_inversión': 'nombre_proyecto',\n    'fecha_de_registro': 'fecha_registro',\n    'ubigeo': 'ubigeo_id',\n    'monto_viable': 'costo_inicial',\n    'costo_actualizado': 'costo_total',\n    'estado_de_la_inversión': 'estado'\n})\n\n# Limpieza de tipos de datos\ndf_proyectos['proyecto_id'] = df_proyectos['proyecto_id'].astype(int)\ndf_proyectos['ubigeo_id'] = df_proyectos['ubigeo_id'].fillna(0).astype(int)\ndf_proyectos['beneficiarios'] = df_proyectos['beneficiarios'].fillna(0).astype(int)"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5979741b",
   "metadata": {
    "id": "5979741b",
    "outputId": "59b356df-c323-42df-8e1b-ef2061f51e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'geografia':\n",
      "   ubigeo_id    region     provincia                 distrito\n",
      "0      10401  AMAZONAS  CONDORCANQUI                    NIEVA\n",
      "1      10000  AMAZONAS     - TODOS -                - TODOS -\n",
      "3      10103  AMAZONAS   CHACHAPOYAS                   BALSAS\n",
      "4      10704  AMAZONAS     UTCUBAMBA               EL MILAGRO\n",
      "5      10118  AMAZONAS   CHACHAPOYAS  SAN FRANCISCO DE DAGUAS\n"
     ]
    }
   ],
   "source": [
    "# Visualizamos las tablas resultantes:\n",
    "# Tabla Geografía\n",
    "print(\"Tabla 'geografia':\")\n",
    "print(df_geografia.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ace6204",
   "metadata": {
    "id": "5ace6204",
    "outputId": "207b8f37-6bc8-43f7-b6fb-d5effb414a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabla 'sectores':\n",
      "                  nombre_sector  sector_id\n",
      "0  TRANSPORTES Y COMUNICACIONES          1\n",
      "1                         SALUD          2\n",
      "2           AGRICULTURA Y RIEGO          3\n",
      "3          GOBIERNOS REGIONALES          4\n",
      "4                     EDUCACION          5\n"
     ]
    }
   ],
   "source": [
    "# Tabla Sectores\n",
    "print(\"\\nTabla 'sectores':\")\n",
    "print(df_sectores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "610b6167",
   "metadata": {
    "id": "610b6167",
    "outputId": "c039c95d-386b-44cf-c292-74f345cc3681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabla 'proyectos':\n",
      "   proyecto_id                                    nombre_proyecto  \\\n",
      "0      2480997  MEJORAMIENTO DE LA CARRETERA PE - 5NC Y PE-5NE...   \n",
      "1      2088578  SEGUNDA FASE DEL PROGRAMA DE APOYO A LA REFORM...   \n",
      "2      2555351  MEJORAMIENTO DE LA CARRETERA PE-5NC, NUEVO SIA...   \n",
      "3      2343984  ERRADICACION DE LA MOSCA DE LA FRUTA EN LOS DE...   \n",
      "4      2630035  CREACION DEL SERVICIO DE TRANSITABILIDAD VIAL ...   \n",
      "\n",
      "  fecha_registro  sector_id  ubigeo_id  costo_inicial   costo_total  estado  \n",
      "0     11/02/2020          1      10401   6.104914e+08  6.562544e+08  ACTIVO  \n",
      "1     21/12/2005          2      10000   4.579201e+08  4.797946e+08  ACTIVO  \n",
      "2     28/06/2022          1      10401   4.122069e+08  4.551198e+08  ACTIVO  \n",
      "3     06/04/2017          3      10103   4.044183e+08  4.455193e+08  ACTIVO  \n",
      "4     18/01/2024          4      10704   3.728068e+08  3.728068e+08  ACTIVO  \n"
     ]
    }
   ],
   "source": [
    "# Tabla Proyectos\n",
    "print(\"\\nTabla 'proyectos':\")\n",
    "print(df_proyectos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03d30f",
   "metadata": {
    "id": "8a03d30f"
   },
   "source": "### Parte 4: Carga a PostgreSQL\n\n> **Prerequisito:** Antes de correr esta celda, abre pgAdmin y ejecuta la sección de creación de tablas del archivo `TRABAJO FINAL - BD INVERSION PUBLICA.sql` (líneas 1–33) para crear la base de datos `inversion_publica_db` y las tablas vacías `geografia`, `sectores` y `proyectos`.\n> \n> Una vez cargados los datos, regresa al archivo SQL para ejecutar las consultas de análisis (sección 2.4)."
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "011072ad",
   "metadata": {
    "id": "011072ad"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1dde58",
   "metadata": {
    "id": "9d1dde58",
    "outputId": "58c4cbab-fe84-42ad-ee6a-df3beb40705e"
   },
   "outputs": [],
   "source": "# ==========================================\n# CARGA A POSTGRESQL\n# ==========================================\n# Ejecutamos SOLO después de haber creado la base de datos \"inversion_publica_db\" y las tablas de Geografia, Sectores y Proyectos con sus respectivas columnas e ID's\n\nload_dotenv()  # Carga las variables del archivo .env\n\nDB_USER = \"postgres\"\nDB_PASSWORD = os.getenv('alnilam')\nDB_HOST = \"localhost\"\nDB_PORT = \"5432\"\nDB_NAME = \"inversion_publica_db\"\n\nif not DB_PASSWORD:\n    raise ValueError(\"No se encontró la variable 'alnilam' en el archivo .env. Verifica que el archivo .env existe y tiene la contraseña.\")\n\ncadena_conexion = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}?client_encoding=utf8'\nengine = create_engine(cadena_conexion)\n\nprint(\"Subiendo a PostgreSQL...\")\ntry:\n    df_geografia.to_sql('geografia', engine, if_exists='append', index=False)\n    df_sectores.to_sql('sectores', engine, if_exists='append', index=False)\n    df_proyectos.to_sql('proyectos', engine, if_exists='append', index=False)\n    print(\"Carga completada exitosamente.\")\nexcept Exception as e:\n    print(f\"Error en la conexión o carga: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3553dac7",
   "metadata": {
    "id": "3553dac7"
   },
   "source": [
    "Verificamos en SQL y empezamos con las operaciones de consulta allá."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}